{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65121685-b4e2-4c31-b211-af74ccac6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit, current_timestamp, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType\n",
    "from utils import Config\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Tuple\n",
    "\n",
    "def align_schemas(df1: DataFrame, df2: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "    \n",
    "    df2 = df2.select(*df2.columns, *[lit(None).alias(col) for col in columns1 - columns2])\n",
    "    df1 = df1.select(*df1.columns, *[lit(None).alias(col) for col in columns2 - columns1])\n",
    "    \n",
    "    all_columns = sorted(list(columns1.union(columns2)))\n",
    "    return df1.select(all_columns), df2.select(all_columns)\n",
    "\n",
    "def read_parquet_with_schema(spark: SparkSession, path: str) -> DataFrame:\n",
    "    return  spark.read.parquet(path)\n",
    "\n",
    "def process_denormalized_model(spark: SparkSession) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    current_day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    input_base_path = f\"{Config.STANDARDIZED_BASE_PATH}/standardized_sales_transaction_{current_day}\"\n",
    "    \n",
    "    online_df = read_parquet_with_schema(spark, f\"{input_base_path}/online_transactions*\")\n",
    "    offline_df = read_parquet_with_schema(spark, f\"{input_base_path}/offline_transactions*\")\n",
    "    \n",
    "    online_df_a = online_df.withColumn(\"transaction_type\", lit(\"online\"))\n",
    "    offline_df_a = offline_df.withColumn(\"transaction_type\", lit(\"offline\"))\n",
    "    \n",
    "    online_df_a, offline_df_a = align_schemas(offline_df_a, online_df_a)\n",
    "    \n",
    "    all_df = online_df_a.unionByName(offline_df_a)\n",
    "    \n",
    "    new_order = [\n",
    "        'transaction_id', 'transaction_date', 'transaction_type', 'customer_id', 'customer_name', 'customer_email',\n",
    "        'product_id', 'product_name', 'product_category', 'units', 'unit_price', 'discount',\n",
    "        'payment_method', 'group', 'sales_agent_id', 'sales_agent_name',\n",
    "        'sales_agent_hire_date', 'branch_id', 'branch_location', 'branch_class',\n",
    "        'shipping_street_name', 'shipping_city', 'shipping_state', 'shipping_zip_code'\n",
    "    ]\n",
    "    \n",
    "    return online_df_a, offline_df_a, all_df.select(new_order)\n",
    "\n",
    "def denorm_modeling(spark: SparkSession, df: DataFrame, transaction_type: str) -> None:\n",
    "    file_path = get_file_path(transaction_type)\n",
    "        \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    merged_file_exists = fs.exists(path)\n",
    "    \n",
    "    print(file_path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(file_path)\n",
    "        changes_records = df.join(existing_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "        merged_df = existing_df.unionByName(changes_records)\n",
    "    else:\n",
    "        merged_df = df\n",
    "    \n",
    "    # Write merged data\n",
    "    merged_df.write.partitionBy(\"transaction_date\") \\\n",
    "                   .option(\"schema\", merged_df.schema.json()) \\\n",
    "                   .mode(\"append\") \\\n",
    "                   .parquet(file_path)\n",
    "    \n",
    "    print(f\"Merged and wrote {merged_df.count()} rows to {file_path}\")\n",
    "\n",
    "def get_file_path(transaction_type: str) -> str:\n",
    "    denorm_path = Config.CONFORMED_DENORMALIZED_BASE_PATH\n",
    "    if transaction_type == 'online':\n",
    "        return f\"{denorm_path}/online_fact_table/online_merged\"\n",
    "    elif transaction_type == 'offline':\n",
    "        return f\"{denorm_path}/offline_fact_table\"\n",
    "    else:\n",
    "        return f\"{denorm_path}/all_sales_fact_table\"\n",
    "\n",
    "# def main():\n",
    "#     spark = SparkSession.builder \\\n",
    "#         .appName(\"DenormalizedModelProcessing\") \\\n",
    "#         .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "#         .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "#         .getOrCreate()\n",
    "    \n",
    "#     try:\n",
    "#         online_df, offline_df, all_df = process_denormalized_model(spark)\n",
    "        \n",
    "#         denorm_modeling(spark, online_df, 'online')\n",
    "#         denorm_modeling(spark, offline_df, 'offline')\n",
    "#         denorm_modeling(spark, all_df, 'all')\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "#     finally:\n",
    "#         spark.stop()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf275e-8788-467b-a2c0-a803f9a39d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:\n",
    "        online_df, offline_df, all_df = process_denormalized_model(spark)\n",
    "        \n",
    "        denorm_modeling(spark, online_df, 'online')\n",
    "        denorm_modeling(spark, offline_df, 'offline')\n",
    "        denorm_modeling(spark, all_df, 'all')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c316a1-5b70-4fc1-ac94-f6a8e2681740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e290ca0c-e050-47a9-a3f0-3f24e4947fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DenormalizedModelProcessing\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46364013-f5e3-44ca-b740-a12f0e681c22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: hdfs://localhost:9000/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o50.parquet.\n: org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://localhost:9000/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:576)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:559)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:559)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:667)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3f101e3a721e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: hdfs://localhost:9000/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged;'"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63e69a2d-a869-4dac-8cf9-0bc72ec52484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from utils import Config\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "def align_schemas(df1: DataFrame, df2: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "    \n",
    "    df2 = df2.select(*df2.columns, *[lit(None).alias(col) for col in columns1 - columns2])\n",
    "    df1 = df1.select(*df1.columns, *[lit(None).alias(col) for col in columns2 - columns1])\n",
    "    \n",
    "    all_columns = sorted(list(columns1.union(columns2)))\n",
    "    return df1.select(all_columns), df2.select(all_columns)\n",
    "\n",
    "def read_parquet_with_schema(spark: SparkSession, path: str) -> DataFrame:\n",
    "    schema_path = os.path.join(path, \"_schema\")\n",
    "    schema_df = spark.read.parquet(schema_path)\n",
    "    schema = schema_df.schema\n",
    "    return spark.read.schema(schema).parquet(path)\n",
    "\n",
    "def process_denormalized_model(spark: SparkSession) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    current_day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    input_base_path = f\"{Config.STANDARDIZED_BASE_PATH}/standardized_sales_transaction_{current_day}\"\n",
    "    \n",
    "    online_df = read_parquet_with_schema(spark, \"/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/online_transactions_group6_20240712192518\")\n",
    "    online_df_a = online_df.withColumn(\"transaction_type\", lit(\"online\"))\n",
    "    \n",
    "    offline_df = read_parquet_with_schema(spark, f\"{input_base_path}/offline_transactions*\")\n",
    "    offline_df_a = offline_df.withColumn(\"transaction_type\", lit(\"offline\"))\n",
    "    \n",
    "    return online_df_a, offline_df_a\n",
    "\n",
    "def get_all_sales(online_df: DataFrame, offline_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    online_df_a, offline_df_a = align_schemas(online_df, offline_df)\n",
    "    \n",
    "    all_df = online_df_a.union(offline_df_a)\n",
    "    \n",
    "    new_order = [\n",
    "        'transaction_id', 'transaction_date', 'transaction_type', 'customer_id', 'customer_name', 'customer_email',\n",
    "        'product_id', 'product_name', 'product_category', 'units', 'unit_price', 'discount',\n",
    "        'payment_method', 'group', 'sales_agent_id', 'sales_agent_name',\n",
    "        'sales_agent_hire_date', 'branch_id', 'branch_location', 'branch_class',\n",
    "        'shipping_street_name', 'shipping_city', 'shipping_state', 'shipping_zip_code'\n",
    "    ]\n",
    "    \n",
    "    return all_df.select(new_order)\n",
    "\n",
    "def denorm_modeling(spark: SparkSession, df: DataFrame, transaction_type: str) -> None:\n",
    "    file_path = get_file_path(transaction_type)\n",
    "    \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    merged_file_exists = fs.exists(path)\n",
    "    print(path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(file_path)\n",
    "        changes_records = df.join(existing_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "        merged_df = existing_df.unionByName(changes_records)\n",
    "    else:\n",
    "        merged_df = df\n",
    "    \n",
    "    print(merged_df.printSchema())\n",
    "    merged_df.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_date\"]).mode(\"append\").parquet(file_path)\n",
    "    print(f\"Appended {df.count()} rows to {file_path}\")\n",
    "\n",
    "def get_file_path(transaction_type: str) -> str:\n",
    "    denorm_path = Config.CONFORMED_DENORMALIZED_BASE_PATH\n",
    "    if transaction_type == 'online':\n",
    "        return f\"{denorm_path}/online_fact_table/online_merged\"\n",
    "    elif transaction_type == 'offline':\n",
    "        return f\"{denorm_path}/offline_fact_table/offline_merged\"\n",
    "    else:\n",
    "        return f\"{denorm_path}/all_sales_fact_table/sales_merged\"\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"DenormalizedModelProcessing\").getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        online_df, offline_df, all_df = process_denormalized_model(spark)\n",
    "        \n",
    "        denorm_modeling(spark, online_df, 'online')\n",
    "        denorm_modeling(spark, offline_df, 'offline')\n",
    "        denorm_modeling(spark, all_df, 'all')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f11038c-9f09-4240-8234-2254c812eb13",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o395.parquet.\n: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\b\n\thdfs://localhost:9000/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/offline_transactions_group6_20240712192719\n\thdfs://localhost:9000/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/offline_transactions_group1_20240712185152\n\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:156)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:100)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:71)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:158)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:667)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bfa309489926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monline_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffline_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_denormalized_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-08dff22dc545>\u001b[0m in \u001b[0;36mprocess_denormalized_model\u001b[0;34m(spark)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0monline_df_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transaction_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"online\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0moffline_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_parquet_with_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{input_base_path}/offline_transactions*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moffline_df_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffline_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transaction_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"offline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-08dff22dc545>\u001b[0m in \u001b[0;36mread_parquet_with_schema\u001b[0;34m(spark, path)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mschema_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_denormalized_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o395.parquet.\n: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\b\n\thdfs://localhost:9000/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/offline_transactions_group6_20240712192719\n\thdfs://localhost:9000/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/offline_transactions_group1_20240712185152\n\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:156)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:100)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:71)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:158)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:667)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "online_df, offline_df = process_denormalized_model(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96d4da8c-d2da-4e94-976f-d64c08e03d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_df = spark.read.parquet(\"/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/online_transactions_group6_20240712192518\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9585685-68df-4be6-ae25-6af75ad82db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "025a464f-9aa5-4fdb-ba42-075ef5e654ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_df=online_df.withColumn(\"transaction_type\", lit(\"online\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb523e4-0520-42f2-8e59-ee2181bb47d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_sales = get_all_sales(online_df, offline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9528427c-4abf-46c3-bbdd-e2ab5f13be40",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- shipping_street_name: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_zip_code: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_type: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aba7006d-5115-4e04-b59b-0c7af7738780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\n",
      "True\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- shipping_street_name: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_zip_code: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 50000 rows to /user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\n"
     ]
    }
   ],
   "source": [
    "denorm_modeling(spark, online_df, 'online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39659978-030b-4298-b0c4-6e980d9c1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21ad1a-5c98-4317-8ddd-49e7d97bd659",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_off_6 = \"/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/offline_transactions_group6_20240712192719\"\n",
    "path_on_6 = \"/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/online_transactions_group6_20240712192518\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbd04653-f1cd-46d2-8669-aaac7bfc8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-12/online_transactions_group1_20240712185034\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b4bea77-c437-4136-914c-0da647a5ef11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67290683-a10b-4903-8bf9-791780777598",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f675e78-a7ba-41b4-99b9-733d2d05abf7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\n",
      "False\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- shipping_street_name: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_zip_code: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_type: string (nullable = false)\n",
      "\n",
      "None\n",
      "Appended 500 rows to /user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\n",
      "/user/itversity/q-company_conformed_layer/denormalized_model/offline_fact_table/offline_merged\n",
      "False\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- sales_agent_id: long (nullable = true)\n",
      " |-- branch_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- sales_agent_name: string (nullable = true)\n",
      " |-- sales_agent_hire_date: date (nullable = true)\n",
      " |-- branch_location: string (nullable = true)\n",
      " |-- branch_establish_date: date (nullable = true)\n",
      " |-- branch_class: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_type: string (nullable = false)\n",
      "\n",
      "None\n",
      "Appended 1000 rows to /user/itversity/q-company_conformed_layer/denormalized_model/offline_fact_table/offline_merged\n",
      "/user/itversity/q-company_conformed_layer/denormalized_model/all_sales_fact_table/sales_merged\n",
      "False\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_type: string (nullable = false)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- sales_agent_id: long (nullable = true)\n",
      " |-- sales_agent_name: string (nullable = true)\n",
      " |-- sales_agent_hire_date: date (nullable = true)\n",
      " |-- branch_id: long (nullable = true)\n",
      " |-- branch_location: string (nullable = true)\n",
      " |-- branch_class: string (nullable = true)\n",
      " |-- shipping_street_name: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_zip_code: string (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 1500 rows to /user/itversity/q-company_conformed_layer/denormalized_model/all_sales_fact_table/sales_merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from utils import Config\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "def get_latest_file(spark, base_path, transaction_type):\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    files = spark.sparkContext.wholeTextFiles(f\"{base_path}/standardized_sales_transaction_{current_date}/{transaction_type}_transactions*\").keys().collect()\n",
    "    \n",
    "    \n",
    "    latest_file = sorted(files, key=lambda x: x.split('_')[-1], reverse=True)[0]\n",
    "    \n",
    "    return latest_file\n",
    "\n",
    "def align_schemas(df1: DataFrame, df2: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "    \n",
    "    df2 = df2.select(*df2.columns, *[lit(None).alias(col) for col in columns1 - columns2])\n",
    "    df1 = df1.select(*df1.columns, *[lit(None).alias(col) for col in columns2 - columns1])\n",
    "    \n",
    "    all_columns = sorted(list(columns1.union(columns2)))\n",
    "    return df1.select(all_columns), df2.select(all_columns)\n",
    "\n",
    "def read_parquet_with_schema(spark: SparkSession, path: str) -> DataFrame:\n",
    "    schema_path = os.path.join(path, \"_schema\")\n",
    "    schema_df = spark.read.parquet(schema_path)\n",
    "    schema = schema_df.schema\n",
    "    return spark.read.schema(schema).parquet(path)\n",
    "\n",
    "def process_denormalized_model(spark: SparkSession) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    base_path = \"/user/itversity/q-company_standardized_layer\"\n",
    "    \n",
    "    # Get latest online and offline files\n",
    "    online_file = get_latest_file(spark, base_path, \"online\", group_name)\n",
    "    offline_file = get_latest_file(spark, base_path, \"offline\", group_name)\n",
    "    \n",
    "    if not online_file or not offline_file:\n",
    "        raise ValueError(f\"Could not find latest files for group {group_name}\")\n",
    "    \n",
    "    online_df = read_parquet_with_schema(spark, online_file)\n",
    "    offline_df = read_parquet_with_schema(spark, offline_file)\n",
    "    \n",
    "    online_df_a = online_df.withColumn(\"transaction_type\", lit(\"online\"))\n",
    "    offline_df_a = offline_df.withColumn(\"transaction_type\", lit(\"offline\"))\n",
    "    \n",
    "    return online_df_a, offline_df_a\n",
    "\n",
    "def get_all_sales(online_df: DataFrame, offline_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    online_df_a, offline_df_a = align_schemas(online_df, offline_df)\n",
    "    \n",
    "    all_df = online_df_a.union(offline_df_a)\n",
    "    \n",
    "    new_order = [\n",
    "        'transaction_id', 'transaction_date', 'transaction_type', 'customer_id', 'customer_name', 'customer_email',\n",
    "        'product_id', 'product_name', 'product_category', 'units', 'unit_price', 'discount',\n",
    "        'payment_method', 'group', 'sales_agent_id', 'sales_agent_name',\n",
    "        'sales_agent_hire_date', 'branch_id', 'branch_location', 'branch_class',\n",
    "        'shipping_street_name', 'shipping_city', 'shipping_state', 'shipping_zip_code'\n",
    "    ]\n",
    "    \n",
    "    return all_df.select(new_order)\n",
    "\n",
    "def denorm_modeling(spark: SparkSession, df: DataFrame, transaction_type: str) -> None:\n",
    "    file_path = get_file_path(transaction_type)\n",
    "    \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    merged_file_exists = fs.exists(path)\n",
    "    print(path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(file_path)\n",
    "        changes_records = df.join(existing_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "        merged_df = existing_df.unionByName(changes_records)\n",
    "    else:\n",
    "        merged_df = df\n",
    "    \n",
    "    print(merged_df.printSchema())\n",
    "    merged_df.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_date\"]).mode(\"append\").parquet(file_path)\n",
    "    print(f\"Appended {df.count()} rows to {file_path}\")\n",
    "\n",
    "def get_file_path(transaction_type: str) -> str:\n",
    "    denorm_path = Config.CONFORMED_DENORMALIZED_BASE_PATH\n",
    "    if transaction_type == 'online':\n",
    "        return f\"{denorm_path}/online_fact_table/online_merged\"\n",
    "    elif transaction_type == 'offline':\n",
    "        return f\"{denorm_path}/offline_fact_table/offline_merged\"\n",
    "    else:\n",
    "        return f\"{denorm_path}/all_sales_fact_table/sales_merged\"\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"DenormalizedModelProcessing\").getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        online_df, offline_df, all_df = process_denormalized_model(spark)\n",
    "        \n",
    "        denorm_modeling(spark, online_df, 'online')\n",
    "        denorm_modeling(spark, offline_df, 'offline')\n",
    "        denorm_modeling(spark, all_df, 'all')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ef5513-c29c-4df5-b04c-980dc8da8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from utils import Config\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "def align_schemas(df1: DataFrame, df2: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "    \n",
    "    df2 = df2.select(*df2.columns, *[lit(None).alias(col) for col in columns1 - columns2])\n",
    "    df1 = df1.select(*df1.columns, *[lit(None).alias(col) for col in columns2 - columns1])\n",
    "    \n",
    "    all_columns = sorted(list(columns1.union(columns2)))\n",
    "    return df1.select(all_columns), df2.select(all_columns)\n",
    "\n",
    "def read_parquet_with_schema(spark: SparkSession, path: str) -> DataFrame:\n",
    "    schema_path = os.path.join(path, \"_schema\")\n",
    "    schema_df = spark.read.parquet(schema_path)\n",
    "    schema = schema_df.schema\n",
    "    return spark.read.schema(schema).parquet(path)\n",
    "\n",
    "def process_denormalized_model(spark: SparkSession) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    current_day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    input_base_path = f\"{Config.STANDARDIZED_BASE_PATH}/standardized_sales_transaction_{current_day}\"\n",
    "    \n",
    "    offline_df = read_parquet_with_schema(spark, \"/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-13/offline_transactions_group5_20240713022953\")\n",
    "    online_df_a = online_df.withColumn(\"transaction_type\", lit(\"online\"))\n",
    "    \n",
    "    online_df = read_parquet_with_schema(spark, \"/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-13/online_transactions_group5_20240713022835\")\n",
    "    offline_df_a = offline_df.withColumn(\"transaction_type\", lit(\"offline\"))\n",
    "    \n",
    "    return online_df_a, offline_df_a\n",
    "\n",
    "def get_all_sales(online_df: DataFrame, offline_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    online_df_a, offline_df_a = align_schemas(online_df, offline_df)\n",
    "    \n",
    "    all_df = online_df_a.union(offline_df_a)\n",
    "    \n",
    "    new_order = [\n",
    "        'transaction_id', 'transaction_date', 'transaction_type', 'customer_id', 'customer_name', 'customer_email',\n",
    "        'product_id', 'product_name', 'product_category', 'units', 'unit_price', 'discount',\n",
    "        'payment_method', 'group', 'sales_agent_id', 'sales_agent_name',\n",
    "        'sales_agent_hire_date', 'branch_id', 'branch_location', 'branch_class',\n",
    "        'shipping_street_name', 'shipping_city', 'shipping_state', 'shipping_zip_code'\n",
    "    ]\n",
    "    \n",
    "    return all_df.select(new_order)\n",
    "\n",
    "def denorm_modeling(spark: SparkSession, df: DataFrame, transaction_type: str) -> None:\n",
    "    file_path = get_file_path(transaction_type)\n",
    "    \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    merged_file_exists = fs.exists(path)\n",
    "    print(path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(file_path)\n",
    "        changes_records = df.join(existing_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "        merged_df = existing_df.unionByName(changes_records)\n",
    "    else:\n",
    "        merged_df = df\n",
    "    \n",
    "    print(merged_df.printSchema())\n",
    "    merged_df.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_date\"]).mode(\"append\").parquet(file_path)\n",
    "    print(f\"Appended {df.count()} rows to {file_path}\")\n",
    "\n",
    "def get_file_path(transaction_type: str) -> str:\n",
    "    denorm_path = Config.CONFORMED_DENORMALIZED_BASE_PATH\n",
    "    if transaction_type == 'online':\n",
    "        return f\"{denorm_path}/online_fact_table/online_merged\"\n",
    "    elif transaction_type == 'offline':\n",
    "        return f\"{denorm_path}/offline_fact_table/offline_merged\"\n",
    "    else:\n",
    "        return f\"{denorm_path}/all_sales_fact_table/sales_merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a54334-9059-4fe4-8af2-cdb66a67f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: local variable 'online_df' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "    spark = SparkSession.builder.appName(\"DenormalizedModelProcessing\").getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        online_df, offline_df, all_df = process_denormalized_model(spark)\n",
    "        \n",
    "        denorm_modeling(spark, online_df, 'online')\n",
    "        denorm_modeling(spark, offline_df, 'offline')\n",
    "        denorm_modeling(spark, all_df, 'all')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3fbdd-df2f-46b7-aa84-a2a07416bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, date_format, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from typing import List, Tuple\n",
    "from utils import Config\n",
    "import os\n",
    "\n",
    "\n",
    "def get_latest_file(spark, base_path, transaction_type):\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    files = spark.sparkContext.wholeTextFiles(f\"{base_path}/standardized_sales_transaction_{current_date}/{transaction_type}_transactions*\").keys().collect()\n",
    "    \n",
    "    \n",
    "    latest_file = sorted(files, key=lambda x: x.split('_')[-1], reverse=True)[0]\n",
    "    \n",
    "    return latest_file\n",
    "\n",
    "def create_spark_session() -> SparkSession:\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Normalized_Model\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def read_parquet(spark: SparkSession, path: str) -> DataFrame:\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "def write_parquet(df: DataFrame, path: str) -> None:\n",
    "    df.write.mode('append').parquet(path)\n",
    "\n",
    "def add_surrogate_key(df: DataFrame, key_name: str, order_by: str) -> DataFrame:\n",
    "    window_spec = Window.orderBy(order_by)\n",
    "    return df.withColumn(key_name, row_number().over(window_spec))\n",
    "\n",
    "def process_date_dimension(spark: SparkSession) -> DataFrame:\n",
    "       return read_parquet(spark, \"/user/itversity/q-company_conformed_layer/normalized_model/date_dim/date_dim_table\")\n",
    "\n",
    "def process_product_dimension(df: DataFrame) -> DataFrame:\n",
    "    product_dim = df.select(\"product_id\", \"product_name\", \"product_category\", \"unit_price\").distinct()\n",
    "    return add_surrogate_key(product_dim, \"product_key\", \"product_id\")\n",
    "\n",
    "def process_customer_dimension(df: DataFrame) -> DataFrame:\n",
    "    customer_dim = df.select(\"customer_id\", \"customer_name\", \"customer_email\").distinct()\n",
    "    return add_surrogate_key(customer_dim, \"customer_key\", \"customer_id\")\n",
    "\n",
    "def process_branch_dimension(df: DataFrame) -> DataFrame:\n",
    "    branch_dim = df.select(\"branch_id\", \"branch_location\", \"branch_establish_date\", \"branch_class\").distinct()\n",
    "    return add_surrogate_key(branch_dim, \"branch_key\", \"branch_id\")\n",
    "\n",
    "def process_sales_agent_dimension(df: DataFrame) -> DataFrame:\n",
    "    sales_agent_dim = df.select(\"sales_agent_id\", \"sales_agent_hire_date\", \"sales_agent_name\").distinct()\n",
    "    return add_surrogate_key(sales_agent_dim, \"sales_agent_key\", \"sales_agent_id\")\n",
    "\n",
    "def process_online_fact(online_df: DataFrame, date_df: DataFrame, product_df: DataFrame, customer_df: DataFrame) -> DataFrame:\n",
    "    online_fact = online_df.select(\n",
    "        \"transaction_id\", \"customer_id\", \"product_id\", \"units\", \"unit_price\", \"discount\",\n",
    "        \"payment_method\", \"group\", \"total_price\", \"shipping_zip_code\", \"shipping_state\",\n",
    "        \"shipping_city\", \"shipping_street_name\", date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "    \n",
    "    # Join with dimension tables\n",
    "    online_fact = online_fact.join(date_df.select('date_key', 'date'), online_fact[\"transaction_date\"] == date_df[\"date\"], \"inner\") \\\n",
    "                             .join(product_df.select('product_key', 'product_id'), \"product_id\") \\\n",
    "                             .join(customer_df.select('customer_key', 'customer_id'), \"customer_id\")\n",
    "        \n",
    "    # Select final columns\n",
    "    return online_fact.select(\n",
    "        \"transaction_id\", \"customer_key\", \"product_key\", \"date_key\", \"units\", \"unit_price\", \"discount\",\n",
    "        \"payment_method\", \"group\", \"total_price\", \"shipping_zip_code\", \"shipping_state\",\n",
    "        \"shipping_city\", \"shipping_street_name\"\n",
    "    )\n",
    "\n",
    "def process_offline_fact(offline_df: DataFrame, date_df: DataFrame, product_df: DataFrame, customer_df: DataFrame, branch_df: DataFrame, sales_agent_df: DataFrame) -> DataFrame:\n",
    "    offline_fact = offline_df.select(\n",
    "        \"transaction_id\", \"customer_id\", \"sales_agent_id\", \"branch_id\", \"product_id\",\n",
    "        \"units\", \"unit_price\", \"discount\", \"payment_method\", \"total_price\",\n",
    "        date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "    \n",
    "    # Join with dimension tables\n",
    "    offline_fact = offline_fact.join(date_df.select('date_key', 'date'), offline_fact[\"transaction_date\"] == date_df[\"date\"], \"inner\") \\\n",
    "                               .join(product_df.select('product_key', 'product_id'), \"product_id\") \\\n",
    "                               .join(customer_df.select('customer_key', 'customer_id'), \"customer_id\") \\\n",
    "                               .join(branch_df.select('branch_key', 'branch_id'), \"branch_id\") \\\n",
    "                               .join(sales_agent_df.select('sales_agent_key', 'sales_agent_id'), \"sales_agent_id\")\n",
    "    \n",
    "    # Select final columns\n",
    "    return offline_fact.select(\n",
    "        \"transaction_id\", \"customer_key\", \"sales_agent_key\", \"branch_key\", \"product_key\", \"date_key\",\n",
    "        \"units\", \"unit_price\", \"discount\", \"payment_method\", \"total_price\", date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "\n",
    "def process_dimensions(all_df: DataFrame, offline_df: DataFrame) -> Tuple[DataFrame, DataFrame, DataFrame, DataFrame, DataFrame]:\n",
    "    date_df = process_date_dimension(spark)\n",
    "    product_df = process_product_dimension(all_df)\n",
    "    customer_df = process_customer_dimension(all_df)\n",
    "    branch_df = process_branch_dimension(offline_df)\n",
    "    sales_agent_df = process_sales_agent_dimension(offline_df)\n",
    "    \n",
    "    return date_df, product_df, customer_df, branch_df, sales_agent_df\n",
    "\n",
    "def save_dimensions(dimensions: List[Tuple[str, DataFrame]]) -> None:\n",
    "    for name, df in dimensions:\n",
    "        write_parquet(df, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/{name}/{name}\")\n",
    "\n",
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    try:\n",
    "        # Read denormalized data\n",
    "        online_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/online_fact_table/online_merged\")\n",
    "        offline_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/offline_fact_table/offline_merged\")\n",
    "        all_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/all_sales_fact_table/sales_merged\")\n",
    "        \n",
    "        # Process dimensions\n",
    "        date_df, product_df, customer_df, branch_df, sales_agent_df = process_dimensions(all_df, offline_df)\n",
    "        \n",
    "        # Save dimensions\n",
    "        dimensions = [\n",
    "            (\"product_dim\", product_df),\n",
    "            (\"customer_dim\", customer_df),\n",
    "            (\"branch_dim\", branch_df),\n",
    "            (\"sales_agent_dim\", sales_agent_df)\n",
    "        ]\n",
    "        save_dimensions(dimensions)\n",
    "        \n",
    "        # Process and save fact tables\n",
    "        online_fact = process_online_fact(online_df, date_df, product_df, customer_df)\n",
    "        offline_fact = process_offline_fact(offline_df, date_df, product_df, customer_df, branch_df, sales_agent_df)\n",
    "        \n",
    "        write_parquet(online_fact, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/online_sales_fact/online_fact\")\n",
    "        write_parquet(offline_fact, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/offline_sales_fact/offline_fact\")\n",
    "        \n",
    "        print(\"Normalized model processing completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fb494a-a0e6-4260-b43b-3b8b3a604f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get denorm data\n",
      "process dim\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/product_dim/product_dim\n",
      "True\n",
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- product_key: integer (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 30 rows to /user/itversity/q-company_conformed_layer/normalized_model/product_dim/product_dim\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/customer_dim/customer_dim\n",
      "True\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_key: integer (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 101 rows to /user/itversity/q-company_conformed_layer/normalized_model/customer_dim/customer_dim\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/branch_dim/branch_dim\n",
      "True\n",
      "root\n",
      " |-- branch_id: long (nullable = true)\n",
      " |-- branch_location: string (nullable = true)\n",
      " |-- branch_establish_date: date (nullable = true)\n",
      " |-- branch_class: string (nullable = true)\n",
      " |-- branch_key: integer (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 6 rows to /user/itversity/q-company_conformed_layer/normalized_model/branch_dim/branch_dim\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/sales_agent_dim/sales_agent_dim\n",
      "True\n",
      "root\n",
      " |-- sales_agent_id: long (nullable = true)\n",
      " |-- sales_agent_hire_date: date (nullable = true)\n",
      " |-- sales_agent_name: string (nullable = true)\n",
      " |-- sales_agent_key: integer (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 11 rows to /user/itversity/q-company_conformed_layer/normalized_model/sales_agent_dim/sales_agent_dim\n",
      "save dim\n",
      "build facts\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/online_sales_fact/online_fact\n",
      "False\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_key: integer (nullable = true)\n",
      " |-- product_key: integer (nullable = true)\n",
      " |-- date_key: long (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- shipping_zip_code: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_street_name: string (nullable = true)\n",
      "\n",
      "None\n",
      "An error occurred: local variable 'changes_records' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, date_format, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from typing import List, Tuple\n",
    "from utils import Config\n",
    "import os\n",
    "\n",
    "\n",
    "def get_latest_file(spark, base_path, transaction_type):\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    files = spark.sparkContext.wholeTextFiles(f\"{base_path}/standardized_sales_transaction_{current_date}/{transaction_type}_transactions*\").keys().collect()\n",
    "    \n",
    "    \n",
    "    latest_file = sorted(files, key=lambda x: x.split('_')[-1], reverse=True)[0]\n",
    "    \n",
    "    return latest_file\n",
    "\n",
    "def create_spark_session() -> SparkSession:\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Normalized_Model\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def read_parquet(spark: SparkSession, path: str) -> DataFrame:\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "def write_parquet(spark, df: DataFrame, path: str, on: str) -> None:\n",
    "    \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    file_path = spark._jvm.org.apache.hadoop.fs.Path(path)\n",
    "    merged_file_exists = fs.exists(file_path)\n",
    "    print(path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(path)\n",
    "        changes_records = df.join(existing_df, on=on, how=\"left_anti\")\n",
    "        merged_df = existing_df.unionByName(changes_records)\n",
    "    else:\n",
    "        merged_df = df\n",
    "    \n",
    "    print(merged_df.printSchema())\n",
    "    \n",
    "    merged_df.write.mode('overwrite').parquet(path)\n",
    "    \n",
    "    print(f\"Appended {df.count()} rows to {file_path}\")\n",
    "\n",
    "def add_surrogate_key(df: DataFrame, key_name: str, order_by: str) -> DataFrame:\n",
    "    window_spec = Window.orderBy(order_by)\n",
    "    return df.withColumn(key_name, row_number().over(window_spec))\n",
    "\n",
    "def process_date_dimension(spark) -> DataFrame:\n",
    "        return read_parquet(spark, \"/user/itversity/q-company_conformed_layer/normalized_model/date_dim/date_dim_table\")\n",
    "\n",
    "def process_product_dimension(df: DataFrame) -> DataFrame:\n",
    "    product_dim = df.select(\"product_id\", \"product_name\", \"product_category\", \"unit_price\").distinct()\n",
    "    return add_surrogate_key(product_dim, \"product_key\", \"product_id\")\n",
    "\n",
    "def process_customer_dimension(df: DataFrame) -> DataFrame:\n",
    "    customer_dim = df.select(\"customer_id\", \"customer_name\", \"customer_email\").distinct()\n",
    "    return add_surrogate_key(customer_dim, \"customer_key\", \"customer_id\")\n",
    "\n",
    "def process_branch_dimension(df: DataFrame) -> DataFrame:\n",
    "    branch_dim = df.select(\"branch_id\", \"branch_location\", \"branch_establish_date\", \"branch_class\").distinct()\n",
    "    return add_surrogate_key(branch_dim, \"branch_key\", \"branch_id\")\n",
    "\n",
    "def process_sales_agent_dimension(df: DataFrame) -> DataFrame:\n",
    "    sales_agent_dim = df.select(\"sales_agent_id\", \"sales_agent_hire_date\", \"sales_agent_name\").distinct()\n",
    "    return add_surrogate_key(sales_agent_dim, \"sales_agent_key\", \"sales_agent_id\")\n",
    "\n",
    "def process_online_fact(online_df: DataFrame, date_df: DataFrame, product_df: DataFrame, customer_df: DataFrame) -> DataFrame:\n",
    "    online_fact = online_df.select(\n",
    "        \"transaction_id\", \"customer_id\", \"product_id\", \"units\", \"unit_price\", \"discount\",\n",
    "        \"payment_method\", \"group\", \"total_price\", \"shipping_zip_code\", \"shipping_state\",\n",
    "        \"shipping_city\", \"shipping_street_name\", date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "    \n",
    "    online_fact = online_fact.join(date_df.select('date_key', 'date'), online_fact[\"transaction_date\"] == date_df[\"date\"], \"inner\") \\\n",
    "                             .join(product_df.select('product_key', 'product_id'), \"product_id\") \\\n",
    "                             .join(customer_df.select('customer_key', 'customer_id'), \"customer_id\")\n",
    "        \n",
    "    return online_fact.select(\n",
    "        \"transaction_id\", \"customer_key\", \"product_key\", \"date_key\", \"units\", \"unit_price\", \"discount\",\n",
    "        \"payment_method\", \"group\", \"total_price\", \"shipping_zip_code\", \"shipping_state\",\n",
    "        \"shipping_city\", \"shipping_street_name\"\n",
    "    )\n",
    "\n",
    "def process_offline_fact(offline_df: DataFrame, date_df: DataFrame, product_df: DataFrame, customer_df: DataFrame, branch_df: DataFrame, sales_agent_df: DataFrame) -> DataFrame:\n",
    "    offline_fact = offline_df.select(\n",
    "        \"transaction_id\", \"customer_id\", \"sales_agent_id\", \"branch_id\", \"product_id\",\n",
    "        \"units\", \"unit_price\", \"discount\", \"payment_method\", \"total_price\",\n",
    "        date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "    \n",
    "    offline_fact = offline_fact.join(date_df.select('date_key', 'date'), offline_fact[\"transaction_date\"] == date_df[\"date\"], \"inner\") \\\n",
    "                               .join(product_df.select('product_key', 'product_id'), \"product_id\") \\\n",
    "                               .join(customer_df.select('customer_key', 'customer_id'), \"customer_id\") \\\n",
    "                               .join(branch_df.select('branch_key', 'branch_id'), \"branch_id\") \\\n",
    "                               .join(sales_agent_df.select('sales_agent_key', 'sales_agent_id'), \"sales_agent_id\")\n",
    "    \n",
    "    return offline_fact.select(\n",
    "        \"transaction_id\", \"customer_key\", \"sales_agent_key\", \"branch_key\", \"product_key\", \"date_key\",\n",
    "        \"units\", \"unit_price\", \"discount\", \"payment_method\", \"total_price\", date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "\n",
    "def process_dimensions(spark: SparkSession, all_df: DataFrame, offline_df: DataFrame) -> Tuple[DataFrame, DataFrame, DataFrame, DataFrame, DataFrame]:\n",
    "    date_df = process_date_dimension(spark)\n",
    "    product_df = process_product_dimension(all_df)\n",
    "    customer_df = process_customer_dimension(all_df)\n",
    "    branch_df = process_branch_dimension(offline_df)\n",
    "    sales_agent_df = process_sales_agent_dimension(offline_df)\n",
    "    \n",
    "    return date_df, product_df, customer_df, branch_df, sales_agent_df\n",
    "\n",
    "def save_dimensions(spark, dimensions: List[Tuple[str, DataFrame]]) -> None:\n",
    "    for name, df, key in dimensions:\n",
    "        write_parquet(spark, df, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/{name}/{name}\", key)\n",
    "\n",
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    try:\n",
    "        online_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/online_fact_table/online_merged\")\n",
    "        offline_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/offline_fact_table/offline_merged\")\n",
    "        all_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/all_sales_fact_table/sales_merged\")\n",
    "        print(\"get denorm data\")\n",
    "        \n",
    "        date_df, product_df, customer_df, branch_df, sales_agent_df = process_dimensions(spark, all_df, offline_df)\n",
    "        print(\"process dim\")\n",
    "        \n",
    "        dimensions = [\n",
    "            (\"product_dim\", product_df, \"product_id\"),\n",
    "            (\"customer_dim\", customer_df, \"customer_id\"),\n",
    "            (\"branch_dim\", branch_df, \"branch_id\"),\n",
    "            (\"sales_agent_dim\", sales_agent_df, \"sales_agent_id\")\n",
    "        ]\n",
    "        save_dimensions(spark, dimensions)\n",
    "        print(\"save dim\")\n",
    "        \n",
    "        online_fact = process_online_fact(online_df, date_df, product_df, customer_df)\n",
    "        offline_fact = process_offline_fact(offline_df, date_df, product_df, customer_df, branch_df, sales_agent_df)\n",
    "        print(\"build facts\")\n",
    "        \n",
    "        write_parquet(spark, online_fact, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/online_sales_fact/online_fact\", \"transaction_id\")\n",
    "        write_parquet(spark, offline_fact, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/offline_sales_fact/offline_fact\", \"transaction_id\")\n",
    "        \n",
    "        print(\"Normalized model processing completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253d354-3c83-4cc2-a89c-450e409e2a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e35ca5d-4131-4180-b5b2-731c9d409409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date: 2024-07-13\n",
      "Searching for files matching pattern: /user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-13/online_transactions*\n",
      "An error occurred: name 'file_paths' is not defined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from utils import Config\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "def get_latest_file(spark, base_path, transaction_type):\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Current date: {current_date}\")\n",
    "    \n",
    "    file_pattern = f\"{base_path}/standardized_sales_transaction_{current_date}/{transaction_type}_transactions*\"\n",
    "    print(f\"Searching for files matching pattern: {file_pattern}\")\n",
    "    \n",
    "    df = spark.read.parquet(file_pattern)\n",
    "    \n",
    "    latest_file = sorted(file_paths, key=lambda x: x.split('_')[-1], reverse=True)[0]\n",
    "    print(f\"Latest file: {latest_file}\")\n",
    "    \n",
    "    return latest_file\n",
    "\n",
    "def align_schemas(df1: DataFrame, df2: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "    \n",
    "    df2 = df2.select(*df2.columns, *[lit(None).alias(col) for col in columns1 - columns2])\n",
    "    df1 = df1.select(*df1.columns, *[lit(None).alias(col) for col in columns2 - columns1])\n",
    "    \n",
    "    all_columns = sorted(list(columns1.union(columns2)))\n",
    "    return df1.select(all_columns), df2.select(all_columns)\n",
    "\n",
    "def read_parquet_with_schema(spark: SparkSession, path: str) -> DataFrame:\n",
    "    schema_path = os.path.join(path, \"_schema\")\n",
    "    schema_df = spark.read.parquet(schema_path)\n",
    "    schema = schema_df.schema\n",
    "    return spark.read.schema(schema).parquet(path)\n",
    "\n",
    "def process_denormalized_model(spark: SparkSession) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    base_path = \"/user/itversity/q-company_standardized_layer\"\n",
    "    \n",
    "    # Get latest online and offline files\n",
    "    online_file = get_latest_file(spark, base_path, \"online\")\n",
    "    offline_file = get_latest_file(spark, base_path, \"offline\")\n",
    "    \n",
    "    online_df = read_parquet_with_schema(spark, online_file)\n",
    "    offline_df = read_parquet_with_schema(spark, offline_file)\n",
    "    \n",
    "    online_df_a = online_df.withColumn(\"transaction_type\", lit(\"online\"))\n",
    "    offline_df_a = offline_df.withColumn(\"transaction_type\", lit(\"offline\"))\n",
    "    \n",
    "    return online_df_a, offline_df_a\n",
    "\n",
    "def get_all_sales(online_df: DataFrame, offline_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    online_df_a, offline_df_a = align_schemas(online_df, offline_df)\n",
    "    \n",
    "    all_df = online_df_a.union(offline_df_a)\n",
    "    \n",
    "    new_order = [\n",
    "        'transaction_id', 'transaction_date', 'transaction_type', 'customer_id', 'customer_name', 'customer_email',\n",
    "        'product_id', 'product_name', 'product_category', 'units', 'unit_price', 'discount',\n",
    "        'payment_method', 'group', 'sales_agent_id', 'sales_agent_name',\n",
    "        'sales_agent_hire_date', 'branch_id', 'branch_location', 'branch_class',\n",
    "        'shipping_street_name', 'shipping_city', 'shipping_state', 'shipping_zip_code'\n",
    "    ]\n",
    "    \n",
    "    return all_df.select(new_order)\n",
    "\n",
    "def denorm_modeling(spark: SparkSession, df: DataFrame, transaction_type: str) -> None:\n",
    "    file_path = get_file_path(transaction_type)\n",
    "    \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    merged_file_exists = fs.exists(path)\n",
    "    print(path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(file_path)\n",
    "        changes_records = df.join(existing_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "        changes_records.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_date\"]).mode(\"append\").parquet(file_path)\n",
    "        print(f\"Appended {changes_records.count()} rows to {file_path}\")\n",
    "    else:\n",
    "        merged_df = df\n",
    "        merged_df.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_date\"]).mode(\"append\").parquet(file_path)\n",
    "        print(f\"Appended {merged_df.count()} rows to {file_path}\")\n",
    "            \n",
    "\n",
    "def get_file_path(transaction_type: str) -> str:\n",
    "    denorm_path = Config.CONFORMED_DENORMALIZED_BASE_PATH\n",
    "    if transaction_type == 'online':\n",
    "        return f\"{denorm_path}/online_fact_table/online_merged\"\n",
    "    elif transaction_type == 'offline':\n",
    "        return f\"{denorm_path}/offline_fact_table/offline_merged\"\n",
    "    else:\n",
    "        return f\"{denorm_path}/all_sales_fact_table/sales_merged\"\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"DenormalizedModelProcessing\").getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        online_df, offline_df, all_df = process_denormalized_model(spark)\n",
    "        \n",
    "        denorm_modeling(spark, online_df, 'online')\n",
    "        denorm_modeling(spark, offline_df, 'offline')\n",
    "        denorm_modeling(spark, all_df, 'all')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321d4c2-9baa-4239-8508-888e93e8c05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae4f3360-bd28-45f3-b854-3fd1aa690c81",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest online file: hdfs://localhost:9000/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-13/online_transactions_group2_20240713063315\n",
      "Latest offline file: hdfs://localhost:9000/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-13/offline_transactions_group2_20240713063418\n",
      "/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\n",
      "True\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- shipping_street_name: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_zip_code: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 500 rows to /user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\n",
      "/user/itversity/q-company_conformed_layer/denormalized_model/offline_fact_table/offline_merged\n",
      "True\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- sales_agent_id: long (nullable = true)\n",
      " |-- branch_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- sales_agent_name: string (nullable = true)\n",
      " |-- sales_agent_hire_date: date (nullable = true)\n",
      " |-- branch_location: string (nullable = true)\n",
      " |-- branch_establish_date: date (nullable = true)\n",
      " |-- branch_class: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-50-6f07ea2de6cc>\", line 129, in main\n",
      "    denorm_modeling(spark, offline_df, 'offline')\n",
      "  File \"<ipython-input-50-6f07ea2de6cc>\", line 110, in denorm_modeling\n",
      "    merged_df.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_id\"]).mode(\"append\").parquet(file_path)\n",
      "  File \"/opt/spark2/python/pyspark/sql/readwriter.py\", line 847, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/spark2/python/pyspark/context.py\", line 270, in signal_handler\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/spark2/python/pyspark/context.py\", line 269, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/spark2/python/pyspark/context.py\", line 1039, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1959.cancelAllJobs.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.GeneratedConstructorAccessor57.newInstance(Unknown Source)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.GeneratedConstructorAccessor57.newInstance(Unknown Source)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.cancelAllJobs(SparkContext.scala:2254)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "/opt/spark2/python/pyspark/context.py:446: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.\n",
      "  RuntimeWarning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6f07ea2de6cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m      \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-6f07ea2de6cc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mdenorm_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'online'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mdenorm_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffline_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'offline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mdenorm_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-6f07ea2de6cc>\u001b[0m in \u001b[0;36mdenorm_modeling\u001b[0;34m(spark, df, transaction_type)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"schema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"transaction_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Appended {df.count()} rows to {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from utils import Config\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "def get_latest_parquet_file(spark, hdfs_path, file_prefix):\n",
    "    try:\n",
    "        # List all files in the HDFS directory\n",
    "        files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
    "                    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(hdfs_path))\n",
    "        \n",
    "        # Filter files by prefix and get only Parquet files\n",
    "        parquet_files = [f.getPath().toString() for f in files if f.getPath().getName().startswith(file_prefix)]\n",
    "        \n",
    "        if not parquet_files:\n",
    "            print(f\"No matching Parquet files found in {hdfs_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Sort the files by modification time\n",
    "        latest_file = max(parquet_files, key=lambda x: os.path.basename(x))\n",
    "        \n",
    "        return latest_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing path {hdfs_path}: {str(e)}\")\n",
    "        print(\"Directory contents:\")\n",
    "        try:\n",
    "            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "            status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(hdfs_path))\n",
    "            for fileStatus in status:\n",
    "                print(fileStatus.getPath().toString())\n",
    "        except Exception as inner_e:\n",
    "            print(f\"Unable to list directory contents: {str(inner_e)}\")\n",
    "        return None\n",
    "\n",
    "def align_schemas(df1: DataFrame, df2: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "    \n",
    "    df2 = df2.select(*df2.columns, *[lit(None).alias(col) for col in columns1 - columns2])\n",
    "    df1 = df1.select(*df1.columns, *[lit(None).alias(col) for col in columns2 - columns1])\n",
    "    \n",
    "    all_columns = sorted(list(columns1.union(columns2)))\n",
    "    return df1.select(all_columns), df2.select(all_columns)\n",
    "\n",
    "def read_parquet_with_schema(spark: SparkSession, path: str) -> DataFrame:\n",
    "    schema_path = os.path.join(path, \"_schema\")\n",
    "    schema_df = spark.read.parquet(schema_path)\n",
    "    schema = schema_df.schema\n",
    "    return spark.read.schema(schema).parquet(path)\n",
    "\n",
    "def process_denormalized_model(spark: SparkSession) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    current_day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    input_base_path = f\"{Config.STANDARDIZED_BASE_PATH}/standardized_sales_transaction_{current_day}\"\n",
    "    \n",
    "    # Get latest online file\n",
    "    online_path = get_latest_parquet_file(spark, input_base_path, \"online_transactions\")\n",
    "    if not online_path:\n",
    "        raise ValueError(f\"No online transaction files found in {input_base_path}\")\n",
    "    print(f\"Latest online file: {online_path}\")\n",
    "    online_df = read_parquet_with_schema(spark, online_path)\n",
    "    online_df_a = online_df.withColumn(\"transaction_type\", lit(\"online\"))\n",
    "    \n",
    "    # Get latest offline file\n",
    "    offline_path = get_latest_parquet_file(spark, input_base_path, \"offline_transactions\")\n",
    "    if not offline_path:\n",
    "        raise ValueError(f\"No offline transaction files found in {input_base_path}\")\n",
    "    print(f\"Latest offline file: {offline_path}\")\n",
    "    offline_df = read_parquet_with_schema(spark, offline_path)\n",
    "    offline_df_a = offline_df.withColumn(\"transaction_type\", lit(\"offline\"))\n",
    "\n",
    "    all_df = get_all_sales(online_df_a, offline_df_a)\n",
    "    \n",
    "    return online_df_a, offline_df_a, all_df\n",
    "\n",
    "def get_all_sales(online_df: DataFrame, offline_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    online_df_a, offline_df_a = align_schemas(online_df, offline_df)\n",
    "    \n",
    "    all_df = online_df_a.union(offline_df_a)\n",
    "    \n",
    "    new_order = [\n",
    "        'transaction_id', 'transaction_date', 'transaction_type', 'customer_id', 'customer_name', 'customer_email',\n",
    "        'product_id', 'product_name', 'product_category', 'units', 'unit_price', 'discount',\n",
    "        'payment_method', 'group', 'sales_agent_id', 'sales_agent_name',\n",
    "        'sales_agent_hire_date', 'branch_id', 'branch_location', 'branch_class',\n",
    "        'shipping_street_name', 'shipping_city', 'shipping_state', 'shipping_zip_code'\n",
    "    ]\n",
    "    \n",
    "    return all_df.select(new_order)\n",
    "\n",
    "def denorm_modeling(spark: SparkSession, df: DataFrame, transaction_type: str) -> None:\n",
    "    file_path = get_file_path(transaction_type)\n",
    "    \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    merged_file_exists = fs.exists(path)\n",
    "    print(path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(file_path)\n",
    "        changes_records = df.join(existing_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "        merged_df = existing_df.unionByName(changes_records)\n",
    "    else:\n",
    "        merged_df = df\n",
    "    \n",
    "    print(merged_df.printSchema())\n",
    "    merged_df.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_date\"]).mode(\"append\").parquet(file_path)\n",
    "    print(f\"Appended {df.count()} rows to {file_path}\")\n",
    "\n",
    "def get_file_path(transaction_type: str) -> str:\n",
    "    denorm_path = Config.CONFORMED_DENORMALIZED_BASE_PATH\n",
    "    if transaction_type == 'online':\n",
    "        return f\"{denorm_path}/online_fact_table/online_merged\"\n",
    "    elif transaction_type == 'offline':\n",
    "        return f\"{denorm_path}/offline_fact_table/offline_merged\"\n",
    "    else:\n",
    "        return f\"{denorm_path}/all_sales_fact_table/sales_merged\"\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"DenormalizedModelProcessing\").getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        online_df, offline_df, all_df = process_denormalized_model(spark)\n",
    "        \n",
    "        denorm_modeling(spark, online_df, 'online')\n",
    "        denorm_modeling(spark, offline_df, 'offline')\n",
    "        denorm_modeling(spark, all_df, 'all')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87297cfc-5704-4390-975d-28abeca048eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest online file: hdfs://localhost:9000/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-13/online_transactions_group2_20240713120140\n",
      "Latest offline file: hdfs://localhost:9000/user/itversity/q-company_standardized_layer/standardized_sales_transaction_2024-07-13/offline_transactions_group2_20240713120344\n",
      "/user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\n",
      "True\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- shipping_street_name: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_zip_code: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 500 rows to /user/itversity/q-company_conformed_layer/denormalized_model/online_fact_table/online_merged\n",
      "/user/itversity/q-company_conformed_layer/denormalized_model/offline_fact_table/offline_merged\n",
      "True\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- sales_agent_id: long (nullable = true)\n",
      " |-- branch_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- sales_agent_name: string (nullable = true)\n",
      " |-- sales_agent_hire_date: date (nullable = true)\n",
      " |-- branch_location: string (nullable = true)\n",
      " |-- branch_establish_date: date (nullable = true)\n",
      " |-- branch_class: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 1000 rows to /user/itversity/q-company_conformed_layer/denormalized_model/offline_fact_table/offline_merged\n",
      "/user/itversity/q-company_conformed_layer/denormalized_model/all_sales_fact_table/sales_merged\n",
      "True\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- units: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: float (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- sales_agent_id: long (nullable = true)\n",
      " |-- sales_agent_name: string (nullable = true)\n",
      " |-- sales_agent_hire_date: date (nullable = true)\n",
      " |-- branch_id: long (nullable = true)\n",
      " |-- branch_location: string (nullable = true)\n",
      " |-- branch_class: string (nullable = true)\n",
      " |-- shipping_street_name: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_zip_code: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      "\n",
      "None\n",
      "Appended 1500 rows to /user/itversity/q-company_conformed_layer/denormalized_model/all_sales_fact_table/sales_merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from Scripts.utils import *\n",
    "\n",
    "def get_latest_parquet_file(spark, hdfs_path, file_prefix):\n",
    "    try:\n",
    "        files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
    "                    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(hdfs_path))\n",
    "        \n",
    "        parquet_files = [f.getPath().toString() for f in files if f.getPath().getName().startswith(file_prefix)]\n",
    "        \n",
    "        if not parquet_files:\n",
    "            print(f\"No matching Parquet files found in {hdfs_path}\")\n",
    "            return None\n",
    "        \n",
    "        #sorted_files = sorted(files, key=lambda f: f.getModificationTime(), reverse=True)\n",
    "        #latest_file = sorted_files[0].getPath().toString() if sorted_files else None\n",
    "        latest_file = max(parquet_files, key=lambda x: os.path.basename(x))\n",
    "        \n",
    "        return latest_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing path {hdfs_path}: {str(e)}\")\n",
    "        print(\"Directory contents:\")\n",
    "        try:\n",
    "            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "            status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(hdfs_path))\n",
    "            for fileStatus in status:\n",
    "                print(fileStatus.getPath().toString())\n",
    "        except Exception as inner_e:\n",
    "            print(f\"Unable to list directory contents: {str(inner_e)}\")\n",
    "        return None\n",
    "\n",
    "def align_schemas(df1: DataFrame, df2: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "    \n",
    "    df2 = df2.select(*df2.columns, *[lit(None).alias(col) for col in columns1 - columns2])\n",
    "    df1 = df1.select(*df1.columns, *[lit(None).alias(col) for col in columns2 - columns1])\n",
    "    \n",
    "    all_columns = sorted(list(columns1.union(columns2)))\n",
    "    return df1.select(all_columns), df2.select(all_columns)\n",
    "\n",
    "def read_parquet_with_schema(spark: SparkSession, path: str) -> DataFrame:\n",
    "    schema_path = os.path.join(path, \"_schema\")\n",
    "    schema_df = spark.read.parquet(schema_path)\n",
    "    schema = schema_df.schema\n",
    "    return spark.read.schema(schema).parquet(path)\n",
    "\n",
    "def process_denormalized_model(spark: SparkSession) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    current_day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    input_base_path = f\"{Config.STANDARDIZED_BASE_PATH}/standardized_sales_transaction_{current_day}\"\n",
    "    \n",
    "    # Get latest online file\n",
    "    online_path = get_latest_parquet_file(spark, input_base_path, \"online_transactions\")\n",
    "    if not online_path:\n",
    "        raise ValueError(f\"No online transaction files found in {input_base_path}\")\n",
    "    print(f\"Latest online file: {online_path}\")\n",
    "    online_df = read_parquet_with_schema(spark, online_path)\n",
    "    online_df_a = online_df.withColumn(\"transaction_type\", lit(\"online\"))\n",
    "    \n",
    "    offline_path = get_latest_parquet_file(spark, input_base_path, \"offline_transactions\")\n",
    "    if not offline_path:\n",
    "        raise ValueError(f\"No offline transaction files found in {input_base_path}\")\n",
    "    print(f\"Latest offline file: {offline_path}\")\n",
    "    offline_df = read_parquet_with_schema(spark, offline_path)\n",
    "    offline_df_a = offline_df.withColumn(\"transaction_type\", lit(\"offline\"))\n",
    "\n",
    "    all_df = get_all_sales(online_df_a, offline_df_a)\n",
    "    \n",
    "    return online_df_a, offline_df_a, all_df\n",
    "\n",
    "def get_all_sales(online_df: DataFrame, offline_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    online_df_a, offline_df_a = align_schemas(online_df, offline_df)\n",
    "    \n",
    "    all_df = online_df_a.union(offline_df_a)\n",
    "    \n",
    "    new_order = [\n",
    "        'transaction_id', 'transaction_date', 'transaction_type', 'customer_id', 'customer_name', 'customer_email',\n",
    "        'product_id', 'product_name', 'product_category', 'units', 'unit_price', 'discount',\n",
    "        'payment_method', 'group', 'sales_agent_id', 'sales_agent_name',\n",
    "        'sales_agent_hire_date', 'branch_id', 'branch_location', 'branch_class',\n",
    "        'shipping_street_name', 'shipping_city', 'shipping_state', 'shipping_zip_code'\n",
    "    ]\n",
    "    \n",
    "    return all_df.select(new_order)\n",
    "\n",
    "def denorm_modeling(spark: SparkSession, df: DataFrame, transaction_type: str) -> None:\n",
    "    file_path = get_file_path(transaction_type)\n",
    "    \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    merged_file_exists = fs.exists(path)\n",
    "    print(path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(file_path)\n",
    "        changes_records = df.join(existing_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "        merged_df = existing_df.unionByName(changes_records)\n",
    "        changes_records.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_date\"]).mode(\"append\").parquet(file_path)\n",
    "    else:\n",
    "        merged_df = df\n",
    "        merged_df.write.option(\"schema\", df.schema.json()).partitionBy([\"transaction_date\"]).mode(\"append\").parquet(file_path)\n",
    "        print(f\"Appended {merged_df.count()} rows to {file_path}\")\n",
    "  \n",
    "    print(merged_df.printSchema())\n",
    "\n",
    "def get_file_path(transaction_type: str) -> str:\n",
    "    denorm_path = Config.CONFORMED_DENORMALIZED_BASE_PATH\n",
    "    if transaction_type == 'online':\n",
    "        return f\"{denorm_path}/online_fact_table/online_merged\"\n",
    "    elif transaction_type == 'offline':\n",
    "        return f\"{denorm_path}/offline_fact_table/offline_merged\"\n",
    "    else:\n",
    "        return f\"{denorm_path}/all_sales_fact_table/sales_merged\"\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"DenormalizedModelProcessing\").getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        online_df, offline_df, all_df = process_denormalized_model(spark)\n",
    "        \n",
    "        denorm_modeling(spark, online_df, 'online')\n",
    "        denorm_modeling(spark, offline_df, 'offline')\n",
    "        denorm_modeling(spark, all_df, 'all')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0432e3-85f8-458e-91e3-053fcfb3510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session() -> SparkSession:\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Normalized_Model\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e02194-399b-4a68-8071-0db2298955f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get denorm data\n",
      "process dim\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/product_dim/product_dim\n",
      "True\n",
      "Appended 0 rows to /user/itversity/q-company_conformed_layer/normalized_model/product_dim/product_dim\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/customer_dim/customer_dim\n",
      "True\n",
      "Appended 0 rows to /user/itversity/q-company_conformed_layer/normalized_model/customer_dim/customer_dim\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/branch_dim/branch_dim\n",
      "True\n",
      "Appended 0 rows to /user/itversity/q-company_conformed_layer/normalized_model/branch_dim/branch_dim\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/sales_agent_dim/sales_agent_dim\n",
      "True\n",
      "Appended 0 rows to /user/itversity/q-company_conformed_layer/normalized_model/sales_agent_dim/sales_agent_dim\n",
      "save dim\n",
      "build facts\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/online_sales_fact/online_fact\n",
      "True\n",
      "Appended 1000 rows to /user/itversity/q-company_conformed_layer/normalized_model/online_sales_fact/online_fact\n",
      "/user/itversity/q-company_conformed_layer/normalized_model/offline_sales_fact/offline_fact\n",
      "True\n",
      "Appended 2000 rows to /user/itversity/q-company_conformed_layer/normalized_model/offline_sales_fact/offline_fact\n",
      "Normalized model processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, date_format, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from typing import List, Tuple\n",
    "from Scripts.utils import Config\n",
    "import os\n",
    "\n",
    "\n",
    "def get_latest_parquet_file(spark, hdfs_path, file_prefix):\n",
    "    try:\n",
    "        files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
    "                    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(hdfs_path))\n",
    "        \n",
    "        parquet_files = [f.getPath().toString() for f in files if f.getPath().getName().startswith(file_prefix)]\n",
    "        \n",
    "        if not parquet_files:\n",
    "            print(f\"No matching Parquet files found in {hdfs_path}\")\n",
    "            return None\n",
    "        \n",
    "        latest_file = max(parquet_files, key=lambda x: os.path.basename(x))\n",
    "        \n",
    "        return latest_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing path {hdfs_path}: {str(e)}\")\n",
    "        print(\"Directory contents:\")\n",
    "        try:\n",
    "            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "            status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(hdfs_path))\n",
    "            for fileStatus in status:\n",
    "                print(fileStatus.getPath().toString())\n",
    "        except Exception as inner_e:\n",
    "            print(f\"Unable to list directory contents: {str(inner_e)}\")\n",
    "        return None\n",
    "\n",
    "def create_spark_session() -> SparkSession:\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Normalized_Model\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def read_parquet(spark: SparkSession, path: str) -> DataFrame:\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "def write_parquet(spark, df: DataFrame, path: str, on: str) -> None:\n",
    "    \n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    file_path = spark._jvm.org.apache.hadoop.fs.Path(path)\n",
    "    merged_file_exists = fs.exists(file_path)\n",
    "    print(path)\n",
    "    print(merged_file_exists)\n",
    "    if merged_file_exists:\n",
    "        existing_df = spark.read.parquet(path)\n",
    "        changes_records = df.join(existing_df, on=on, how=\"left_anti\")\n",
    "        changes_records.write.mode('append').parquet(path)\n",
    "        print(f\"Appended {changes_records.count()} rows to {file_path}\")\n",
    "    else:\n",
    "        merged_df = df\n",
    "        merged_df.write.mode('overwrite').parquet(path)\n",
    "        print(f\"Appended {merged_df.count()} rows to {file_path}\")\n",
    "        print(merged_df.printSchema())\n",
    "\n",
    "    \n",
    "\n",
    "def add_surrogate_key(df: DataFrame, key_name: str, order_by: str) -> DataFrame:\n",
    "    window_spec = Window.orderBy(order_by)\n",
    "    return df.withColumn(key_name, row_number().over(window_spec))\n",
    "\n",
    "def process_date_dimension(spark) -> DataFrame:\n",
    "        return read_parquet(spark, \"/user/itversity/q-company_conformed_layer/normalized_model/date_dim/date_dim_table\")\n",
    "\n",
    "def process_product_dimension(df: DataFrame) -> DataFrame:\n",
    "    product_dim = df.select(\"product_id\", \"product_name\", \"product_category\", \"unit_price\").distinct()\n",
    "    return add_surrogate_key(product_dim, \"product_key\", \"product_id\")\n",
    "\n",
    "def process_customer_dimension(df: DataFrame) -> DataFrame:\n",
    "    customer_dim = df.select(\"customer_id\", \"customer_name\", \"customer_email\").distinct()\n",
    "    return add_surrogate_key(customer_dim, \"customer_key\", \"customer_id\")\n",
    "\n",
    "def process_branch_dimension(df: DataFrame) -> DataFrame:\n",
    "    branch_dim = df.select(\"branch_id\", \"branch_location\", \"branch_establish_date\", \"branch_class\").distinct()\n",
    "    return add_surrogate_key(branch_dim, \"branch_key\", \"branch_id\")\n",
    "\n",
    "def process_sales_agent_dimension(df: DataFrame) -> DataFrame:\n",
    "    sales_agent_dim = df.select(\"sales_agent_id\", \"sales_agent_hire_date\", \"sales_agent_name\").distinct()\n",
    "    return add_surrogate_key(sales_agent_dim, \"sales_agent_key\", \"sales_agent_id\")\n",
    "\n",
    "def process_online_fact(online_df: DataFrame, date_df: DataFrame, product_df: DataFrame, customer_df: DataFrame) -> DataFrame:\n",
    "    online_fact = online_df.select(\n",
    "        \"transaction_id\", \"customer_id\", \"product_id\", \"units\", \"unit_price\", \"discount\",\n",
    "        \"payment_method\", \"group\", \"total_price\", \"shipping_zip_code\", \"shipping_state\",\n",
    "        \"shipping_city\", \"shipping_street_name\", date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "    \n",
    "    online_fact = online_fact.join(date_df.select('date_key', 'date'), online_fact[\"transaction_date\"] == date_df[\"date\"], \"inner\") \\\n",
    "                             .join(product_df.select('product_key', 'product_id'), \"product_id\") \\\n",
    "                             .join(customer_df.select('customer_key', 'customer_id'), \"customer_id\")\n",
    "        \n",
    "    return online_fact.select(\n",
    "        \"transaction_id\", \"customer_key\", \"product_key\", \"date_key\", \"units\", \"unit_price\", \"discount\",\n",
    "        \"payment_method\", \"group\", \"total_price\", \"shipping_zip_code\", \"shipping_state\",\n",
    "        \"shipping_city\", \"shipping_street_name\"\n",
    "    )\n",
    "\n",
    "def process_offline_fact(offline_df: DataFrame, date_df: DataFrame, product_df: DataFrame, customer_df: DataFrame, branch_df: DataFrame, sales_agent_df: DataFrame) -> DataFrame:\n",
    "    offline_fact = offline_df.select(\n",
    "        \"transaction_id\", \"customer_id\", \"sales_agent_id\", \"branch_id\", \"product_id\",\n",
    "        \"units\", \"unit_price\", \"discount\", \"payment_method\", \"total_price\",\n",
    "        date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "    \n",
    "    offline_fact = offline_fact.join(date_df.select('date_key', 'date'), offline_fact[\"transaction_date\"] == date_df[\"date\"], \"inner\") \\\n",
    "                               .join(product_df.select('product_key', 'product_id'), \"product_id\") \\\n",
    "                               .join(customer_df.select('customer_key', 'customer_id'), \"customer_id\") \\\n",
    "                               .join(branch_df.select('branch_key', 'branch_id'), \"branch_id\") \\\n",
    "                               .join(sales_agent_df.select('sales_agent_key', 'sales_agent_id'), \"sales_agent_id\")\n",
    "    \n",
    "    return offline_fact.select(\n",
    "        \"transaction_id\", \"customer_key\", \"sales_agent_key\", \"branch_key\", \"product_key\", \"date_key\",\n",
    "        \"units\", \"unit_price\", \"discount\", \"payment_method\", \"total_price\", date_format(\"transaction_date\", \"yyyy-MM-dd\").alias(\"transaction_date\")\n",
    "    )\n",
    "\n",
    "def process_dimensions(spark: SparkSession, all_df: DataFrame, offline_df: DataFrame) -> Tuple[DataFrame, DataFrame, DataFrame, DataFrame, DataFrame]:\n",
    "    date_df = process_date_dimension(spark)\n",
    "    product_df = process_product_dimension(all_df)\n",
    "    customer_df = process_customer_dimension(all_df)\n",
    "    branch_df = process_branch_dimension(offline_df)\n",
    "    sales_agent_df = process_sales_agent_dimension(offline_df)\n",
    "    \n",
    "    return date_df, product_df, customer_df, branch_df, sales_agent_df\n",
    "\n",
    "def save_dimensions(spark, dimensions: List[Tuple[str, DataFrame]]) -> None:\n",
    "    for name, df, key in dimensions:\n",
    "        write_parquet(spark, df, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/{name}/{name}\", key)\n",
    "\n",
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    try:\n",
    "        online_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/online_fact_table/online_merged\")\n",
    "        offline_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/offline_fact_table/offline_merged\")\n",
    "        all_df = read_parquet(spark, f\"{Config.CONFORMED_DENORMALIZED_BASE_PATH}/all_sales_fact_table/sales_merged\")\n",
    "        print(\"get denorm data\")\n",
    "        \n",
    "        date_df, product_df, customer_df, branch_df, sales_agent_df = process_dimensions(spark, all_df, offline_df)\n",
    "        print(\"process dim\")\n",
    "        \n",
    "        dimensions = [\n",
    "            (\"product_dim\", product_df, \"product_id\"),\n",
    "            (\"customer_dim\", customer_df, \"customer_id\"),\n",
    "            (\"branch_dim\", branch_df, \"branch_id\"),\n",
    "            (\"sales_agent_dim\", sales_agent_df, \"sales_agent_id\")\n",
    "        ]\n",
    "        save_dimensions(spark, dimensions)\n",
    "        print(\"save dim\")\n",
    "        \n",
    "        online_fact = process_online_fact(online_df, date_df, product_df, customer_df)\n",
    "        offline_fact = process_offline_fact(offline_df, date_df, product_df, customer_df, branch_df, sales_agent_df)\n",
    "        print(\"build facts\")\n",
    "        \n",
    "        write_parquet(spark, online_fact, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/online_sales_fact/online_fact\", \"transaction_id\")\n",
    "        write_parquet(spark, offline_fact, f\"{Config.CONFORMED_NORMALIZED_BASE_PATH}/offline_sales_fact/offline_fact\", \"transaction_id\")\n",
    "        \n",
    "        print(\"Normalized model processing completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef4681db-4bae-47d2-92e1-17e21bf51b83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdcbfe1-56dc-4479-9d1a-0944203bcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
