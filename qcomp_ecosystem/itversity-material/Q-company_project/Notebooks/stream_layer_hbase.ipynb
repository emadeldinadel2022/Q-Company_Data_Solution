{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622134de-77a9-4e73-b844-b4e77eae49f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting happybase\n",
      "  Downloading happybase-1.2.0.tar.gz (40 kB)\n",
      "     |################################| 40 kB 206 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from happybase) (1.11.0)\n",
      "Collecting thriftpy2>=0.4\n",
      "  Downloading thriftpy2-0.5.2.tar.gz (782 kB)\n",
      "     |################################| 782 kB 282 kB/s            \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[33m  WARNING: Missing build requirements in pyproject.toml for thriftpy2>=0.4 from https://files.pythonhosted.org/packages/f8/3a/d983b26df17583a3cc865a9e1737bb8faacfa1e16e3ed17353ef48847e6b/thriftpy2-0.5.2.tar.gz#sha256=cefcb2f6f8b12c00054c6f942dd2323a53b48b8b6862312d03b677dcf0d4a6da (from happybase).\u001b[0m\n",
      "\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[33m  WARNING: Generating metadata for package thriftpy2 produced metadata for project name unknown. Fix your #egg=thriftpy2 fragments.\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f8/3a/d983b26df17583a3cc865a9e1737bb8faacfa1e16e3ed17353ef48847e6b/thriftpy2-0.5.2.tar.gz#sha256=cefcb2f6f8b12c00054c6f942dd2323a53b48b8b6862312d03b677dcf0d4a6da (from https://pypi.org/simple/thriftpy2/) (requires-python:>=3.6). Requested unknown from https://files.pythonhosted.org/packages/f8/3a/d983b26df17583a3cc865a9e1737bb8faacfa1e16e3ed17353ef48847e6b/thriftpy2-0.5.2.tar.gz#sha256=cefcb2f6f8b12c00054c6f942dd2323a53b48b8b6862312d03b677dcf0d4a6da (from happybase) has inconsistent name: filename has 'thriftpy2', but metadata has 'unknown'\u001b[0m\n",
      "\u001b[?25h  Downloading thriftpy2-0.5.1.tar.gz (781 kB)\n",
      "     |################################| 781 kB 215 kB/s            \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[33m  WARNING: Missing build requirements in pyproject.toml for thriftpy2>=0.4 from https://files.pythonhosted.org/packages/27/23/a0a3d09e0f98d96200156ae4173a58372e659cef5b9d785f0838a27b1513/thriftpy2-0.5.1.tar.gz#sha256=27068fb5dbb5959b4c1f5a6ba5b1afbf5e8e4e744310e9633307be8174051c0f (from happybase).\u001b[0m\n",
      "\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ply<4.0,>=3.4\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "     |################################| 49 kB 256 kB/s            \n",
      "\u001b[?25hCollecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: happybase, thriftpy2\n",
      "  Building wheel for happybase (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for happybase: filename=happybase-1.2.0-py2.py3-none-any.whl size=23491 sha256=ec1963da8bf0d8fdb5741852c57bf8440de09a28e8b4879ecb1b388d78444b26\n",
      "  Stored in directory: /home/itversity/.cache/pip/wheels/ee/78/c9/092e37a61812bae659556db5f294ac2facb9a830587c806f89\n",
      "  Building wheel for thriftpy2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for thriftpy2: filename=thriftpy2-0.5.1-cp36-cp36m-linux_x86_64.whl size=1534468 sha256=a158bd000bdf9922ebaaafeb98dbbe4446565bde89a68f872f7aada82206d50f\n",
      "  Stored in directory: /home/itversity/.cache/pip/wheels/49/d9/ea/93f37e068e4dda61f5437968212b5d68e44c68a83f530e47cb\n",
      "Successfully built happybase thriftpy2\n",
      "Installing collected packages: six, ply, thriftpy2, happybase\n",
      "Successfully installed happybase-1.2.0 ply-3.11 six-1.16.0 thriftpy2-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install happybase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44af3e89-c19f-4b5b-8eaf-9c36dd6c82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, TimestampType, DoubleType\n",
    "from happybase import Connection\n",
    "import sys\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc0c1648-20f2-4a5e-9384-1087c7b08bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://itvdelab:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>KafkaStreamingToHBase</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b46e49b42e8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamingToHBase\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.8,org.apache.hbase:hbase-client:2.4.9,org.apache.hbase:hbase-common:2.4.9\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c2f3141-95d8-4558-b183-01f9bce44962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the incoming JSON data\n",
    "schema = StructType() \\\n",
    "    .add(\"eventType\", StringType()) \\\n",
    "    .add(\"customerId\", StringType()) \\\n",
    "    .add(\"productId\", StringType()) \\\n",
    "    .add(\"timestamp\", TimestampType()) \\\n",
    "    .add(\"metadata\", StructType()\n",
    "        .add(\"category\", StringType())\n",
    "        .add(\"source\", StringType())\n",
    "    ) \\\n",
    "    .add(\"quantity\", IntegerType()) \\\n",
    "    .add(\"totalAmount\", DoubleType()) \\\n",
    "    .add(\"paymentMethod\", StringType()) \\\n",
    "    .add(\"recommendedProductId\", StringType()) \\\n",
    "    .add(\"algorithm\", StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a618ab15-b2d3-40be-9816-fd5cdd6faeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka connection details\n",
    "bootstrap_servers = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "kafka_topic = \"Emad_topic\" # add topic name\n",
    "kafka_username = \"JUKQQM4ZM632RECA\"\n",
    "kafka_password = \"UUkrPuSttgOC0U9lY3ZansNsKfN9fbxZPFwrGxudDrfv+knTD4rCwK+KdIzVPX0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "361138eb-c444-43b5-8d72-4908304f38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\",\n",
    "            f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_username}\" password=\"{kafka_password}\";') \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0643c63f-3253-4ad6-b453-45e4bc7a1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = df.selectExpr(\"CAST(value AS STRING)\").select(from_json(\"value\", schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10bc3c84-ca24-4a86-a7c7-cb50e105cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToHBase(df, epoch_id):\n",
    "    print(f\"Batch {epoch_id}: {df.count()} records\")\n",
    "    df.show(5, truncate=False)  # Show first 5 records\n",
    "\n",
    "    rdd = df.rdd\n",
    "    \n",
    "    rdd.foreachPartition(lambda rows: writePartition(rows))\n",
    "\n",
    "def writePartition(rows):\n",
    "    \n",
    "    hbase_host = 'hbase'\n",
    "    hbase_table = 'CUSTEVENTS:CustomerEvents'\n",
    "    \n",
    "    try:\n",
    "        connection = Connection(hbase_host)\n",
    "        table = connection.table(hbase_table)\n",
    "        print(f\"Successfully connected to HBase and opened table {hbase_table}\", file=sys.stderr)\n",
    "        print(f\"Available tables: {tables}\", file=sys.stderr)\n",
    "        \n",
    "        if hbase_table.encode() not in tables:\n",
    "            print(f\"Table {hbase_table} does not exist!\", file=sys.stderr)\n",
    "            return\n",
    "        \n",
    "        row_count = 0\n",
    "        for row in rows:\n",
    "            try:\n",
    "                row_key = row['customerId']\n",
    "                data = row.asDict()\n",
    "                hbase_data = {f'cf:{k}'.encode(): str(v).encode() for k, v in data.items()}\n",
    "                table.put(row_key.encode(), hbase_data)\n",
    "                row_count += 1\n",
    "            except Exception as row_error:\n",
    "                print(f\"Error processing row: {str(row_error)}\", file=sys.stderr)\n",
    "                print(f\"Row data: {row}\", file=sys.stderr)\n",
    "                traceback.print_exc(file=sys.stderr)\n",
    "        \n",
    "        print(f\"Wrote {row_count} rows to HBase\", file=sys.stderr)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in writePartition: {str(e)}\", file=sys.stderr)\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "    finally:\n",
    "        if 'connection' in locals():\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7895d004-b80b-492c-9c6c-acdb5c0a9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = json_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(writeToHBase) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df65d6f3-43e2-46e8-8678-ab52218cfa42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: 537 records\n",
      "+-------------------+----------+---------+-------------------+---------------------+--------+-----------+-------------+--------------------+-------------+\n",
      "|eventType          |customerId|productId|timestamp          |metadata             |quantity|totalAmount|paymentMethod|recommendedProductId|algorithm    |\n",
      "+-------------------+----------+---------+-------------------+---------------------+--------+-----------+-------------+--------------------+-------------+\n",
      "|productView        |68829     |3432     |2024-07-09 11:15:27|[Clothing, Search]   |null    |null       |null         |null                |null         |\n",
      "|productView        |30516     |6488     |2024-07-09 11:15:29|[Electronics, Direct]|null    |null       |null         |null                |null         |\n",
      "|recommendationClick|45014     |7822     |2024-07-09 11:15:31|[,]                  |null    |null       |null         |6637                |content_based|\n",
      "|productView        |78939     |8505     |2024-07-09 11:15:32|[Clothing, Direct]   |null    |null       |null         |null                |null         |\n",
      "|recommendationClick|13219     |4558     |2024-07-09 11:15:33|[,]                  |null    |null       |null         |3776                |content_based|\n",
      "+-------------------+----------+---------+-------------------+---------------------+--------+-----------+-------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 191, in call\\n    raise e\\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 188, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"<ipython-input-40-6e62a8a5909d>\", line 7, in writeToHBase\\n    rdd.foreachPartition(lambda rows: writePartition(rows))\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 806, in foreachPartition\\n    self.mapPartitions(func).count()  # Force evaluation\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 1055, in count\\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 1046, in sum\\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 917, in fold\\n    vals = self.mapPartitions(func).collect()\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 816, in collect\\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\\n    answer, self.gateway_client, self.target_id, self.name)\\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 63, in deco\\n    return f(*a, **kw)\\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\\n    format(target_id, \".\", name), value)\\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 10, itvdelab, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\\n  File \"/opt/spark2/python/pyspark/worker.py\", line 364, in main\\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\\n  File \"/opt/spark2/python/pyspark/worker.py\", line 69, in read_command\\n    command = serializer._read_with_length(file)\\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 173, in _read_with_length\\n    return self.loads(obj)\\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 587, in loads\\n    return pickle.loads(obj, encoding=encoding)\\nModuleNotFoundError: No module named \\'happybase\\'\\n\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\\n\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n\\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\\n\\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\\n\\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\\n\\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\\n\\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\\n\\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\\n\\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\\n\\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\\n\\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\\n\\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\\n\\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\\nDriver stacktrace:\\n\\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\\n\\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\\n\\tat scala.Option.foreach(Option.scala:257)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\\n\\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\\n\\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\\n\\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\\n  File \"/opt/spark2/python/pyspark/worker.py\", line 364, in main\\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\\n  File \"/opt/spark2/python/pyspark/worker.py\", line 69, in read_command\\n    command = serializer._read_with_length(file)\\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 173, in _read_with_length\\n    return self.loads(obj)\\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 587, in loads\\n    return pickle.loads(obj, encoding=encoding)\\nModuleNotFoundError: No module named \\'happybase\\'\\n\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\\n\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n\\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\\n\\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\\n\\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\\n\\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\\n\\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\\n\\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\\n\\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\\n\\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\\n\\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\\n\\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\\n\\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\n\\n\\n=== Streaming Query ===\\nIdentifier: [id = 6bdec090-76a3-4ed8-9185-d05444a58758, runId = b9c87492-35e9-4ea7-9765-fd53bd975bf7]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {KafkaV2[Subscribe[Emad_topic]]: {\"Emad_topic\":{\"1\":274,\"0\":263}}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nProject [data#321.eventType AS eventType#323, data#321.customerId AS customerId#324, data#321.productId AS productId#325, data#321.timestamp AS timestamp#326, data#321.metadata AS metadata#327, data#321.quantity AS quantity#328, data#321.totalAmount AS totalAmount#329, data#321.paymentMethod AS paymentMethod#330, data#321.recommendedProductId AS recommendedProductId#331, data#321.algorithm AS algorithm#332]\\n+- Project [jsontostructs(StructField(eventType,StringType,true), StructField(customerId,StringType,true), StructField(productId,StringType,true), StructField(timestamp,TimestampType,true), StructField(metadata,StructType(StructField(category,StringType,true), StructField(source,StringType,true)),true), StructField(quantity,IntegerType,true), StructField(totalAmount,DoubleType,true), StructField(paymentMethod,StringType,true), StructField(recommendedProductId,StringType,true), StructField(algorithm,StringType,true), value#319, Some(GMT)) AS data#321]\\n   +- Project [cast(value#306 as string) AS value#319]\\n      +- StreamingExecutionRelation KafkaV2[Subscribe[Emad_topic]], [key#305, value#306, topic#307, partition#308, offset#309L, timestamp#310, timestampType#311]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o474.awaitTermination.\n: org.apache.spark.sql.streaming.StreamingQueryException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 191, in call\n    raise e\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 188, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"<ipython-input-40-6e62a8a5909d>\", line 7, in writeToHBase\n    rdd.foreachPartition(lambda rows: writePartition(rows))\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 806, in foreachPartition\n    self.mapPartitions(func).count()  # Force evaluation\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 1055, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 1046, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 917, in fold\n    vals = self.mapPartitions(func).collect()\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 816, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 10, itvdelab, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2/python/pyspark/worker.py\", line 364, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/opt/spark2/python/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 587, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'happybase'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2/python/pyspark/worker.py\", line 364, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/opt/spark2/python/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 587, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'happybase'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n=== Streaming Query ===\nIdentifier: [id = 6bdec090-76a3-4ed8-9185-d05444a58758, runId = b9c87492-35e9-4ea7-9765-fd53bd975bf7]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[Emad_topic]]: {\"Emad_topic\":{\"1\":274,\"0\":263}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [data#321.eventType AS eventType#323, data#321.customerId AS customerId#324, data#321.productId AS productId#325, data#321.timestamp AS timestamp#326, data#321.metadata AS metadata#327, data#321.quantity AS quantity#328, data#321.totalAmount AS totalAmount#329, data#321.paymentMethod AS paymentMethod#330, data#321.recommendedProductId AS recommendedProductId#331, data#321.algorithm AS algorithm#332]\n+- Project [jsontostructs(StructField(eventType,StringType,true), StructField(customerId,StringType,true), StructField(productId,StringType,true), StructField(timestamp,TimestampType,true), StructField(metadata,StructType(StructField(category,StringType,true), StructField(source,StringType,true)),true), StructField(quantity,IntegerType,true), StructField(totalAmount,DoubleType,true), StructField(paymentMethod,StringType,true), StructField(recommendedProductId,StringType,true), StructField(algorithm,StringType,true), value#319, Some(GMT)) AS data#321]\n   +- Project [cast(value#306 as string) AS value#319]\n      +- StreamingExecutionRelation KafkaV2[Subscribe[Emad_topic]], [key#305, value#306, topic#307, partition#308, offset#309L, timestamp#310, timestampType#311]\n\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:297)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)\nCaused by: py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 191, in call\n    raise e\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 188, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"<ipython-input-40-6e62a8a5909d>\", line 7, in writeToHBase\n    rdd.foreachPartition(lambda rows: writePartition(rows))\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 806, in foreachPartition\n    self.mapPartitions(func).count()  # Force evaluation\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 1055, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 1046, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 917, in fold\n    vals = self.mapPartitions(func).collect()\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 816, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 10, itvdelab, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2/python/pyspark/worker.py\", line 364, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/opt/spark2/python/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 587, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'happybase'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2/python/pyspark/worker.py\", line 364, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/opt/spark2/python/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 587, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'happybase'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n\tat py4j.Protocol.getReturnValue(Protocol.java:473)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n\tat com.sun.proxy.$Proxy26.call(Unknown Source)\n\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)\n\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)\n\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$19.apply(MicroBatchExecution.scala:548)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:546)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:545)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)\n\t... 1 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-885fef5a9f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: 'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 191, in call\\n    raise e\\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 188, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"<ipython-input-40-6e62a8a5909d>\", line 7, in writeToHBase\\n    rdd.foreachPartition(lambda rows: writePartition(rows))\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 806, in foreachPartition\\n    self.mapPartitions(func).count()  # Force evaluation\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 1055, in count\\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 1046, in sum\\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 917, in fold\\n    vals = self.mapPartitions(func).collect()\\n  File \"/opt/spark2/python/pyspark/rdd.py\", line 816, in collect\\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\\n    answer, self.gateway_client, self.target_id, self.name)\\n  File \"/opt/spark2/python/pyspark/sql/utils.py\", line 63, in deco\\n    return f(*a, **kw)\\n  File \"/opt/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\\n    format(target_id, \".\", name), value)\\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 10, itvdelab, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\\n  File \"/opt/spark2/python/pyspark/worker.py\", line 364, in main\\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\\n  File \"/opt/spark2/python/pyspark/worker.py\", line 69, in read_command\\n    command = serializer._read_with_length(file)\\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 173, in _read_with_length\\n    return self.loads(obj)\\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 587, in loads\\n    return pickle.loads(obj, encoding=encoding)\\nModuleNotFoundError: No module named \\'happybase\\'\\n\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\\n\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n\\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\\n\\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\\n\\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\\n\\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\\n\\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\\n\\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\\n\\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\\n\\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\\n\\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\\n\\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\\n\\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\\nDriver stacktrace:\\n\\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\\n\\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\\n\\tat scala.Option.foreach(Option.scala:257)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\\n\\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\\n\\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\\n\\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\\n  File \"/opt/spark2/python/pyspark/worker.py\", line 364, in main\\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\\n  File \"/opt/spark2/python/pyspark/worker.py\", line 69, in read_command\\n    command = serializer._read_with_length(file)\\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 173, in _read_with_length\\n    return self.loads(obj)\\n  File \"/opt/spark2/python/pyspark/serializers.py\", line 587, in loads\\n    return pickle.loads(obj, encoding=encoding)\\nModuleNotFoundError: No module named \\'happybase\\'\\n\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\\n\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n\\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\\n\\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\\n\\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\\n\\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\\n\\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\\n\\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\\n\\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\\n\\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\\n\\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\\n\\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\\n\\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\n\\n\\n=== Streaming Query ===\\nIdentifier: [id = 6bdec090-76a3-4ed8-9185-d05444a58758, runId = b9c87492-35e9-4ea7-9765-fd53bd975bf7]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {KafkaV2[Subscribe[Emad_topic]]: {\"Emad_topic\":{\"1\":274,\"0\":263}}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nProject [data#321.eventType AS eventType#323, data#321.customerId AS customerId#324, data#321.productId AS productId#325, data#321.timestamp AS timestamp#326, data#321.metadata AS metadata#327, data#321.quantity AS quantity#328, data#321.totalAmount AS totalAmount#329, data#321.paymentMethod AS paymentMethod#330, data#321.recommendedProductId AS recommendedProductId#331, data#321.algorithm AS algorithm#332]\\n+- Project [jsontostructs(StructField(eventType,StringType,true), StructField(customerId,StringType,true), StructField(productId,StringType,true), StructField(timestamp,TimestampType,true), StructField(metadata,StructType(StructField(category,StringType,true), StructField(source,StringType,true)),true), StructField(quantity,IntegerType,true), StructField(totalAmount,DoubleType,true), StructField(paymentMethod,StringType,true), StructField(recommendedProductId,StringType,true), StructField(algorithm,StringType,true), value#319, Some(GMT)) AS data#321]\\n   +- Project [cast(value#306 as string) AS value#319]\\n      +- StreamingExecutionRelation KafkaV2[Subscribe[Emad_topic]], [key#305, value#306, topic#307, partition#308, offset#309L, timestamp#310, timestampType#311]\\n'"
     ]
    }
   ],
   "source": [
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bdc3bf7-6262-4380-aec9-afc33c3937a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c6b4d5d9db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e19c27-1385-45ab-ba8f-18cbfa473fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
